# Interpretability_Methods_on_CNNs

In this project, I implemented and explained three interpretability methods on Convolutional Neural Networks: Class Activation Map (CAM), Gradient-Weighted CAM (Grad-CAM), and Saliency Map. 
These methods help identify the regions (features) of an input image that have the most impact on a CNN's predictions. 

The famous VGG16 network and cats_vs_dogs dataset are used in this project. Since the network was originally trained on the ImageNet dataset, I applied transfer learning and modified the network's top layers to adopt it to the cats_vs_dogs dataset.
